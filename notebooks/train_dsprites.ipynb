{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = collections.namedtuple('Args', 'config resume device')\n",
    "config = ConfigParser.from_args(Args(config='config.json', resume=None, device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dataset: KeysView(<numpy.lib.npyio.NpzFile object at 0x7f6079f54730>)\n",
      "Dataset loaded : OK.\n"
     ]
    }
   ],
   "source": [
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = getattr(module_loss, config['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = config.init_obj('optimizer', torch.optim, trainable_params)\n",
    "\n",
    "lr_scheduler = config.init_obj('lr_scheduler', torch.optim.lr_scheduler, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, criterion, [], optimizer, config=config,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/663552 (0%)] Loss: 21.565903\n",
      "Train Epoch: 1 [92160/663552 (14%)] Loss: -638.385254\n",
      "Train Epoch: 1 [184320/663552 (28%)] Loss: -11059.964844\n",
      "Train Epoch: 1 [276480/663552 (42%)] Loss: -11110.362305\n",
      "Train Epoch: 1 [368640/663552 (56%)] Loss: -11116.131836\n",
      "Train Epoch: 1 [460800/663552 (69%)] Loss: -11118.238281\n",
      "Train Epoch: 1 [552960/663552 (83%)] Loss: -11118.792969\n",
      "Train Epoch: 1 [645120/663552 (97%)] Loss: -11119.078125\n",
      "    epoch          : 1\n",
      "    loss           : -9142.08513000865\n",
      "    val_loss       : -11119.130479600695\n",
      "Train Epoch: 2 [0/663552 (0%)] Loss: -11119.262695\n",
      "Train Epoch: 2 [92160/663552 (14%)] Loss: -11119.357422\n",
      "Train Epoch: 2 [184320/663552 (28%)] Loss: -11119.303711\n",
      "Train Epoch: 2 [276480/663552 (42%)] Loss: -11119.426758\n",
      "Train Epoch: 2 [368640/663552 (56%)] Loss: -11119.638672\n",
      "Train Epoch: 2 [460800/663552 (69%)] Loss: -11119.441406\n",
      "Train Epoch: 2 [552960/663552 (83%)] Loss: -11119.559570\n",
      "Train Epoch: 2 [645120/663552 (97%)] Loss: -11119.626953\n",
      "    epoch          : 2\n",
      "    loss           : -11119.443718050734\n",
      "    val_loss       : -11119.644965277777\n",
      "Train Epoch: 3 [0/663552 (0%)] Loss: -11119.660156\n",
      "Train Epoch: 3 [92160/663552 (14%)] Loss: -11119.728516\n",
      "Train Epoch: 3 [184320/663552 (28%)] Loss: -11119.880859\n",
      "Train Epoch: 3 [276480/663552 (42%)] Loss: -11120.041992\n",
      "Train Epoch: 3 [368640/663552 (56%)] Loss: -11120.228516\n",
      "Train Epoch: 3 [460800/663552 (69%)] Loss: -11120.076172\n",
      "Train Epoch: 3 [552960/663552 (83%)] Loss: -11119.679688\n",
      "Train Epoch: 3 [645120/663552 (97%)] Loss: -11119.853516\n",
      "    epoch          : 3\n",
      "    loss           : -11119.913519965277\n",
      "    val_loss       : -11120.032118055555\n",
      "Train Epoch: 4 [0/663552 (0%)] Loss: -11120.247070\n",
      "Train Epoch: 4 [92160/663552 (14%)] Loss: -11119.898438\n",
      "Train Epoch: 4 [184320/663552 (28%)] Loss: -11119.997070\n",
      "Train Epoch: 4 [276480/663552 (42%)] Loss: -11119.880859\n",
      "Train Epoch: 4 [368640/663552 (56%)] Loss: -11120.228516\n",
      "Train Epoch: 4 [460800/663552 (69%)] Loss: -11120.355469\n",
      "Train Epoch: 4 [552960/663552 (83%)] Loss: -11120.110352\n",
      "Train Epoch: 4 [645120/663552 (97%)] Loss: -11120.240234\n",
      "    epoch          : 4\n",
      "    loss           : -11120.117531105325\n",
      "    val_loss       : -11120.259657118055\n",
      "Train Epoch: 5 [0/663552 (0%)] Loss: -11120.315430\n",
      "Train Epoch: 5 [92160/663552 (14%)] Loss: -11120.060547\n",
      "Train Epoch: 5 [184320/663552 (28%)] Loss: -11120.284180\n",
      "Train Epoch: 5 [276480/663552 (42%)] Loss: -11120.612305\n",
      "Train Epoch: 5 [368640/663552 (56%)] Loss: -11120.608398\n",
      "Train Epoch: 5 [460800/663552 (69%)] Loss: -11120.438477\n",
      "Train Epoch: 5 [552960/663552 (83%)] Loss: -11120.489258\n",
      "Train Epoch: 5 [645120/663552 (97%)] Loss: -11120.453125\n",
      "    epoch          : 5\n",
      "    loss           : -11120.441638334298\n",
      "    val_loss       : -11120.550021701389\n",
      "Train Epoch: 6 [0/663552 (0%)] Loss: -11120.515625\n",
      "Train Epoch: 6 [92160/663552 (14%)] Loss: -11120.558594\n",
      "Train Epoch: 6 [184320/663552 (28%)] Loss: -11120.691406\n",
      "Train Epoch: 6 [276480/663552 (42%)] Loss: -11120.623047\n",
      "Train Epoch: 6 [368640/663552 (56%)] Loss: -11120.579102\n",
      "Train Epoch: 6 [460800/663552 (69%)] Loss: -11120.611328\n",
      "Train Epoch: 6 [552960/663552 (83%)] Loss: -11120.747070\n",
      "Train Epoch: 6 [645120/663552 (97%)] Loss: -11120.719727\n",
      "    epoch          : 6\n",
      "    loss           : -11120.600932556907\n",
      "    val_loss       : -11120.673529730902\n",
      "Train Epoch: 7 [0/663552 (0%)] Loss: -11120.653320\n",
      "Train Epoch: 7 [92160/663552 (14%)] Loss: -11120.494141\n",
      "Train Epoch: 7 [184320/663552 (28%)] Loss: -11120.710938\n",
      "Train Epoch: 7 [276480/663552 (42%)] Loss: -11120.603516\n",
      "Train Epoch: 7 [368640/663552 (56%)] Loss: -11121.025391\n",
      "Train Epoch: 7 [460800/663552 (69%)] Loss: -11120.733398\n",
      "Train Epoch: 7 [552960/663552 (83%)] Loss: -11120.738281\n",
      "Train Epoch: 7 [645120/663552 (97%)] Loss: -11121.051758\n",
      "    epoch          : 7\n",
      "    loss           : -11120.734504605516\n",
      "    val_loss       : -11120.824490017361\n",
      "Train Epoch: 8 [0/663552 (0%)] Loss: -11120.756836\n",
      "Train Epoch: 8 [92160/663552 (14%)] Loss: -11120.717773\n",
      "Train Epoch: 8 [184320/663552 (28%)] Loss: -11120.720703\n",
      "Train Epoch: 8 [276480/663552 (42%)] Loss: -11120.840820\n",
      "Train Epoch: 8 [368640/663552 (56%)] Loss: -11120.895508\n",
      "Train Epoch: 8 [460800/663552 (69%)] Loss: -11120.969727\n",
      "Train Epoch: 8 [552960/663552 (83%)] Loss: -11120.697266\n",
      "Train Epoch: 8 [645120/663552 (97%)] Loss: -11120.925781\n",
      "    epoch          : 8\n",
      "    loss           : -11120.921480155286\n",
      "    val_loss       : -11121.141791449652\n",
      "Train Epoch: 9 [0/663552 (0%)] Loss: -11120.953125\n",
      "Train Epoch: 9 [92160/663552 (14%)] Loss: -11121.108398\n",
      "Train Epoch: 9 [184320/663552 (28%)] Loss: -11121.181641\n",
      "Train Epoch: 9 [276480/663552 (42%)] Loss: -11121.207031\n",
      "Train Epoch: 9 [368640/663552 (56%)] Loss: -11121.153320\n",
      "Train Epoch: 9 [460800/663552 (69%)] Loss: -11121.062500\n",
      "Train Epoch: 9 [552960/663552 (83%)] Loss: -11121.276367\n",
      "Train Epoch: 9 [645120/663552 (97%)] Loss: -11121.119141\n",
      "    epoch          : 9\n",
      "    loss           : -11121.170087649498\n",
      "    val_loss       : -11121.227376302084\n",
      "Train Epoch: 10 [0/663552 (0%)] Loss: -11121.198242\n",
      "Train Epoch: 10 [92160/663552 (14%)] Loss: -11121.306641\n",
      "Train Epoch: 10 [184320/663552 (28%)] Loss: -11121.272461\n",
      "Train Epoch: 10 [276480/663552 (42%)] Loss: -11121.092773\n",
      "Train Epoch: 10 [368640/663552 (56%)] Loss: -11121.204102\n",
      "Train Epoch: 10 [460800/663552 (69%)] Loss: -11121.195312\n",
      "Train Epoch: 10 [552960/663552 (83%)] Loss: -11121.369141\n",
      "Train Epoch: 10 [645120/663552 (97%)] Loss: -11121.362305\n",
      "    epoch          : 10\n",
      "    loss           : -11121.265335648148\n",
      "    val_loss       : -11121.3310546875\n",
      "Saving checkpoint: saved/models/Shapes_ChyVAE/0802_175221/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [0/663552 (0%)] Loss: -11121.558594\n",
      "Train Epoch: 11 [92160/663552 (14%)] Loss: -11121.305664\n",
      "Train Epoch: 11 [184320/663552 (28%)] Loss: -11121.118164\n",
      "Train Epoch: 11 [276480/663552 (42%)] Loss: -11121.406250\n",
      "Train Epoch: 11 [368640/663552 (56%)] Loss: -11121.414062\n",
      "Train Epoch: 11 [460800/663552 (69%)] Loss: -11121.205078\n",
      "Train Epoch: 11 [552960/663552 (83%)] Loss: -11121.362305\n",
      "Train Epoch: 11 [645120/663552 (97%)] Loss: -11121.436523\n",
      "    epoch          : 11\n",
      "    loss           : -11121.383303795332\n",
      "    val_loss       : -11121.475558810764\n",
      "Train Epoch: 12 [0/663552 (0%)] Loss: -11121.592773\n",
      "Train Epoch: 12 [92160/663552 (14%)] Loss: -11121.201172\n",
      "Train Epoch: 12 [184320/663552 (28%)] Loss: -11121.488281\n",
      "Train Epoch: 12 [276480/663552 (42%)] Loss: -11121.344727\n",
      "Train Epoch: 12 [368640/663552 (56%)] Loss: -11121.583984\n",
      "Train Epoch: 12 [460800/663552 (69%)] Loss: -11121.677734\n",
      "Train Epoch: 12 [552960/663552 (83%)] Loss: -11121.691406\n",
      "Train Epoch: 12 [645120/663552 (97%)] Loss: -11121.791016\n",
      "    epoch          : 12\n",
      "    loss           : -11121.55818986304\n",
      "    val_loss       : -11121.680962456598\n",
      "Train Epoch: 13 [0/663552 (0%)] Loss: -11121.673828\n",
      "Train Epoch: 13 [92160/663552 (14%)] Loss: -11121.701172\n",
      "Train Epoch: 13 [184320/663552 (28%)] Loss: -11121.581055\n",
      "Train Epoch: 13 [276480/663552 (42%)] Loss: -11121.673828\n",
      "Train Epoch: 13 [368640/663552 (56%)] Loss: -11121.895508\n",
      "Train Epoch: 13 [460800/663552 (69%)] Loss: -11121.917969\n",
      "Train Epoch: 13 [552960/663552 (83%)] Loss: -11121.911133\n",
      "Train Epoch: 13 [645120/663552 (97%)] Loss: -11121.867188\n",
      "    epoch          : 13\n",
      "    loss           : -11121.760956187307\n",
      "    val_loss       : -11121.84632703993\n",
      "Train Epoch: 14 [0/663552 (0%)] Loss: -11121.726562\n",
      "Train Epoch: 14 [92160/663552 (14%)] Loss: -11121.854492\n",
      "Train Epoch: 14 [184320/663552 (28%)] Loss: -11121.705078\n",
      "Train Epoch: 14 [276480/663552 (42%)] Loss: -11121.918945\n",
      "Train Epoch: 14 [368640/663552 (56%)] Loss: -11122.076172\n",
      "Train Epoch: 14 [460800/663552 (69%)] Loss: -11121.936523\n",
      "Train Epoch: 14 [552960/663552 (83%)] Loss: -11121.978516\n",
      "Train Epoch: 14 [645120/663552 (97%)] Loss: -11122.044922\n",
      "    epoch          : 14\n",
      "    loss           : -11121.867766203704\n",
      "    val_loss       : -11121.949490017361\n",
      "Train Epoch: 15 [0/663552 (0%)] Loss: -11121.969727\n",
      "Train Epoch: 15 [92160/663552 (14%)] Loss: -11121.854492\n",
      "Train Epoch: 15 [184320/663552 (28%)] Loss: -11121.964844\n",
      "Train Epoch: 15 [276480/663552 (42%)] Loss: -11121.873047\n",
      "Train Epoch: 15 [368640/663552 (56%)] Loss: -11122.125977\n",
      "Train Epoch: 15 [460800/663552 (69%)] Loss: -11121.875000\n",
      "Train Epoch: 15 [552960/663552 (83%)] Loss: -11122.107422\n",
      "Train Epoch: 15 [645120/663552 (97%)] Loss: -11122.355469\n",
      "    epoch          : 15\n",
      "    loss           : -11121.9913104022\n",
      "    val_loss       : -11122.050672743055\n",
      "Train Epoch: 16 [0/663552 (0%)] Loss: -11122.124023\n",
      "Train Epoch: 16 [92160/663552 (14%)] Loss: -11122.206055\n",
      "Train Epoch: 16 [184320/663552 (28%)] Loss: -11122.092773\n",
      "Train Epoch: 16 [276480/663552 (42%)] Loss: -11122.050781\n",
      "Train Epoch: 16 [368640/663552 (56%)] Loss: -11121.936523\n",
      "Train Epoch: 16 [460800/663552 (69%)] Loss: -11122.239258\n",
      "Train Epoch: 16 [552960/663552 (83%)] Loss: -11122.049805\n",
      "Train Epoch: 16 [645120/663552 (97%)] Loss: -11122.128906\n",
      "    epoch          : 16\n",
      "    loss           : -11122.1133144049\n",
      "    val_loss       : -11122.192708333334\n",
      "Train Epoch: 17 [0/663552 (0%)] Loss: -11121.978516\n",
      "Train Epoch: 17 [92160/663552 (14%)] Loss: -11122.160156\n",
      "Train Epoch: 17 [184320/663552 (28%)] Loss: -11122.258789\n",
      "Train Epoch: 17 [276480/663552 (42%)] Loss: -11122.256836\n",
      "Train Epoch: 17 [368640/663552 (56%)] Loss: -11122.270508\n",
      "Train Epoch: 17 [460800/663552 (69%)] Loss: -11122.470703\n",
      "Train Epoch: 17 [552960/663552 (83%)] Loss: -11121.931641\n",
      "Train Epoch: 17 [645120/663552 (97%)] Loss: -11122.378906\n",
      "    epoch          : 17\n",
      "    loss           : -11122.258237485532\n",
      "    val_loss       : -11122.332411024305\n",
      "Train Epoch: 18 [0/663552 (0%)] Loss: -11122.378906\n",
      "Train Epoch: 18 [92160/663552 (14%)] Loss: -11122.264648\n",
      "Train Epoch: 18 [184320/663552 (28%)] Loss: -11122.401367\n",
      "Train Epoch: 18 [276480/663552 (42%)] Loss: -11122.350586\n",
      "Train Epoch: 18 [368640/663552 (56%)] Loss: -11122.508789\n",
      "Train Epoch: 18 [460800/663552 (69%)] Loss: -11122.337891\n",
      "Train Epoch: 18 [552960/663552 (83%)] Loss: -11122.448242\n",
      "Train Epoch: 18 [645120/663552 (97%)] Loss: -11122.466797\n",
      "    epoch          : 18\n",
      "    loss           : -11122.373372395834\n",
      "    val_loss       : -11122.434353298611\n",
      "Train Epoch: 19 [0/663552 (0%)] Loss: -11122.586914\n",
      "Train Epoch: 19 [92160/663552 (14%)] Loss: -11122.260742\n",
      "Train Epoch: 19 [184320/663552 (28%)] Loss: -11122.657227\n",
      "Train Epoch: 19 [276480/663552 (42%)] Loss: -11122.438477\n",
      "Train Epoch: 19 [368640/663552 (56%)] Loss: -11122.491211\n",
      "Train Epoch: 19 [460800/663552 (69%)] Loss: -11122.609375\n",
      "Train Epoch: 19 [552960/663552 (83%)] Loss: -11122.500000\n",
      "Train Epoch: 19 [645120/663552 (97%)] Loss: -11122.588867\n",
      "    epoch          : 19\n",
      "    loss           : -11122.468554084684\n",
      "    val_loss       : -11122.530870225695\n",
      "Train Epoch: 20 [0/663552 (0%)] Loss: -11122.626953\n",
      "Train Epoch: 20 [92160/663552 (14%)] Loss: -11122.706055\n",
      "Train Epoch: 20 [184320/663552 (28%)] Loss: -11122.570312\n",
      "Train Epoch: 20 [276480/663552 (42%)] Loss: -11122.496094\n",
      "Train Epoch: 20 [368640/663552 (56%)] Loss: -11122.546875\n",
      "Train Epoch: 20 [460800/663552 (69%)] Loss: -11122.544922\n",
      "Train Epoch: 20 [552960/663552 (83%)] Loss: -11122.639648\n",
      "Train Epoch: 20 [645120/663552 (97%)] Loss: -11122.540039\n",
      "    epoch          : 20\n",
      "    loss           : -11122.574098186727\n",
      "    val_loss       : -11122.64146592882\n",
      "Saving checkpoint: saved/models/Shapes_ChyVAE/0802_175221/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [0/663552 (0%)] Loss: -11122.691406\n",
      "Train Epoch: 21 [92160/663552 (14%)] Loss: -11122.698242\n",
      "Train Epoch: 21 [184320/663552 (28%)] Loss: -11122.727539\n",
      "Train Epoch: 21 [276480/663552 (42%)] Loss: -11122.779297\n",
      "Train Epoch: 21 [368640/663552 (56%)] Loss: -11122.687500\n",
      "Train Epoch: 21 [460800/663552 (69%)] Loss: -11122.595703\n",
      "Train Epoch: 21 [552960/663552 (83%)] Loss: -11122.726562\n",
      "Train Epoch: 21 [645120/663552 (97%)] Loss: -11122.772461\n",
      "    epoch          : 21\n",
      "    loss           : -11122.691150053048\n",
      "    val_loss       : -11122.760633680555\n",
      "Train Epoch: 22 [0/663552 (0%)] Loss: -11122.733398\n",
      "Train Epoch: 22 [92160/663552 (14%)] Loss: -11122.876953\n",
      "Train Epoch: 22 [184320/663552 (28%)] Loss: -11122.812500\n",
      "Train Epoch: 22 [276480/663552 (42%)] Loss: -11122.742188\n",
      "Train Epoch: 22 [368640/663552 (56%)] Loss: -11122.842773\n",
      "Train Epoch: 22 [460800/663552 (69%)] Loss: -11122.534180\n",
      "Train Epoch: 22 [552960/663552 (83%)] Loss: -11122.934570\n",
      "Train Epoch: 22 [645120/663552 (97%)] Loss: -11122.973633\n",
      "    epoch          : 22\n",
      "    loss           : -11122.795015311536\n",
      "    val_loss       : -11122.845024956598\n",
      "Train Epoch: 23 [0/663552 (0%)] Loss: -11122.976562\n",
      "Train Epoch: 23 [92160/663552 (14%)] Loss: -11122.586914\n",
      "Train Epoch: 23 [184320/663552 (28%)] Loss: -11122.892578\n",
      "Train Epoch: 23 [276480/663552 (42%)] Loss: -11122.921875\n",
      "Train Epoch: 23 [368640/663552 (56%)] Loss: -11122.996094\n",
      "Train Epoch: 23 [460800/663552 (69%)] Loss: -11122.849609\n",
      "Train Epoch: 23 [552960/663552 (83%)] Loss: -11122.887695\n",
      "Train Epoch: 23 [645120/663552 (97%)] Loss: -11122.769531\n",
      "    epoch          : 23\n",
      "    loss           : -11122.861569251543\n",
      "    val_loss       : -11122.894938151041\n",
      "Train Epoch: 24 [0/663552 (0%)] Loss: -11122.773438\n",
      "Train Epoch: 24 [92160/663552 (14%)] Loss: -11122.955078\n",
      "Train Epoch: 24 [184320/663552 (28%)] Loss: -11122.992188\n",
      "Train Epoch: 24 [276480/663552 (42%)] Loss: -11122.980469\n",
      "Train Epoch: 24 [368640/663552 (56%)] Loss: -11122.749023\n",
      "Train Epoch: 24 [460800/663552 (69%)] Loss: -11122.925781\n",
      "Train Epoch: 24 [552960/663552 (83%)] Loss: -11122.833008\n",
      "Train Epoch: 24 [645120/663552 (97%)] Loss: -11123.051758\n",
      "    epoch          : 24\n",
      "    loss           : -11122.897436824845\n",
      "    val_loss       : -11122.919270833334\n",
      "Train Epoch: 25 [0/663552 (0%)] Loss: -11122.985352\n",
      "Train Epoch: 25 [92160/663552 (14%)] Loss: -11123.014648\n",
      "Train Epoch: 25 [184320/663552 (28%)] Loss: -11122.803711\n",
      "Train Epoch: 25 [276480/663552 (42%)] Loss: -11122.887695\n",
      "Train Epoch: 25 [368640/663552 (56%)] Loss: -11123.126953\n",
      "Train Epoch: 25 [460800/663552 (69%)] Loss: -11122.742188\n",
      "Train Epoch: 25 [552960/663552 (83%)] Loss: -11122.904297\n",
      "Train Epoch: 25 [645120/663552 (97%)] Loss: -11122.941406\n",
      "    epoch          : 25\n",
      "    loss           : -11122.916072892554\n",
      "    val_loss       : -11122.93443467882\n",
      "Train Epoch: 26 [0/663552 (0%)] Loss: -11122.911133\n",
      "Train Epoch: 26 [92160/663552 (14%)] Loss: -11122.824219\n",
      "Train Epoch: 26 [184320/663552 (28%)] Loss: -11122.978516\n",
      "Train Epoch: 26 [276480/663552 (42%)] Loss: -11122.666992\n",
      "Train Epoch: 26 [368640/663552 (56%)] Loss: -11122.945312\n",
      "Train Epoch: 26 [460800/663552 (69%)] Loss: -11122.929688\n",
      "Train Epoch: 26 [552960/663552 (83%)] Loss: -11123.050781\n",
      "Train Epoch: 26 [645120/663552 (97%)] Loss: -11122.857422\n",
      "    epoch          : 26\n",
      "    loss           : -11122.926923586998\n",
      "    val_loss       : -11122.94251844618\n",
      "Train Epoch: 27 [0/663552 (0%)] Loss: -11122.963867\n",
      "Train Epoch: 27 [92160/663552 (14%)] Loss: -11122.863281\n",
      "Train Epoch: 27 [184320/663552 (28%)] Loss: -11123.047852\n",
      "Train Epoch: 27 [276480/663552 (42%)] Loss: -11122.791016\n",
      "Train Epoch: 27 [368640/663552 (56%)] Loss: -11122.878906\n",
      "Train Epoch: 27 [460800/663552 (69%)] Loss: -11122.876953\n",
      "Train Epoch: 27 [552960/663552 (83%)] Loss: -11122.837891\n",
      "Train Epoch: 27 [645120/663552 (97%)] Loss: -11123.080078\n",
      "    epoch          : 27\n",
      "    loss           : -11122.933777608989\n",
      "    val_loss       : -11122.947374131945\n",
      "Train Epoch: 28 [0/663552 (0%)] Loss: -11123.212891\n",
      "Train Epoch: 28 [92160/663552 (14%)] Loss: -11122.890625\n",
      "Train Epoch: 28 [184320/663552 (28%)] Loss: -11122.933594\n",
      "Train Epoch: 28 [276480/663552 (42%)] Loss: -11123.072266\n",
      "Train Epoch: 28 [368640/663552 (56%)] Loss: -11123.011719\n",
      "Train Epoch: 28 [460800/663552 (69%)] Loss: -11123.035156\n",
      "Train Epoch: 28 [552960/663552 (83%)] Loss: -11123.022461\n",
      "Train Epoch: 28 [645120/663552 (97%)] Loss: -11122.953125\n",
      "    epoch          : 28\n",
      "    loss           : -11122.937861689816\n",
      "    val_loss       : -11122.950303819445\n",
      "Train Epoch: 29 [0/663552 (0%)] Loss: -11123.082031\n",
      "Train Epoch: 29 [92160/663552 (14%)] Loss: -11122.872070\n",
      "Train Epoch: 29 [184320/663552 (28%)] Loss: -11123.063477\n",
      "Train Epoch: 29 [276480/663552 (42%)] Loss: -11122.869141\n",
      "Train Epoch: 29 [368640/663552 (56%)] Loss: -11122.898438\n",
      "Train Epoch: 29 [460800/663552 (69%)] Loss: -11122.875000\n",
      "Train Epoch: 29 [552960/663552 (83%)] Loss: -11122.965820\n",
      "Train Epoch: 29 [645120/663552 (97%)] Loss: -11122.900391\n",
      "    epoch          : 29\n",
      "    loss           : -11122.940215687693\n",
      "    val_loss       : -11122.952690972223\n",
      "Train Epoch: 30 [0/663552 (0%)] Loss: -11123.146484\n",
      "Train Epoch: 30 [92160/663552 (14%)] Loss: -11123.241211\n",
      "Train Epoch: 30 [184320/663552 (28%)] Loss: -11122.828125\n",
      "Train Epoch: 30 [276480/663552 (42%)] Loss: -11122.950195\n",
      "Train Epoch: 30 [368640/663552 (56%)] Loss: -11123.008789\n",
      "Train Epoch: 30 [460800/663552 (69%)] Loss: -11123.017578\n",
      "Train Epoch: 30 [552960/663552 (83%)] Loss: -11123.077148\n",
      "Train Epoch: 30 [645120/663552 (97%)] Loss: -11123.150391\n",
      "    epoch          : 30\n",
      "    loss           : -11122.942250192902\n",
      "    val_loss       : -11122.954399956598\n",
      "Saving checkpoint: saved/models/Shapes_ChyVAE/0802_175221/checkpoint-epoch30.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 31 [0/663552 (0%)] Loss: -11122.906250\n",
      "Train Epoch: 31 [92160/663552 (14%)] Loss: -11123.110352\n",
      "Train Epoch: 31 [184320/663552 (28%)] Loss: -11123.018555\n",
      "Train Epoch: 31 [276480/663552 (42%)] Loss: -11122.923828\n",
      "Train Epoch: 31 [368640/663552 (56%)] Loss: -11122.862305\n",
      "Train Epoch: 31 [460800/663552 (69%)] Loss: -11122.817383\n",
      "Train Epoch: 31 [552960/663552 (83%)] Loss: -11122.886719\n",
      "Train Epoch: 31 [645120/663552 (97%)] Loss: -11123.031250\n",
      "    epoch          : 31\n",
      "    loss           : -11122.943582417052\n",
      "    val_loss       : -11122.9560546875\n",
      "Train Epoch: 32 [0/663552 (0%)] Loss: -11122.942383\n",
      "Train Epoch: 32 [92160/663552 (14%)] Loss: -11122.875000\n",
      "Train Epoch: 32 [184320/663552 (28%)] Loss: -11122.937500\n",
      "Train Epoch: 32 [276480/663552 (42%)] Loss: -11122.860352\n",
      "Train Epoch: 32 [368640/663552 (56%)] Loss: -11122.927734\n",
      "Train Epoch: 32 [460800/663552 (69%)] Loss: -11122.827148\n",
      "Train Epoch: 32 [552960/663552 (83%)] Loss: -11122.969727\n",
      "Train Epoch: 32 [645120/663552 (97%)] Loss: -11122.990234\n",
      "    epoch          : 32\n",
      "    loss           : -11122.944745852623\n",
      "    val_loss       : -11122.956814236111\n",
      "Train Epoch: 33 [0/663552 (0%)] Loss: -11123.022461\n",
      "Train Epoch: 33 [92160/663552 (14%)] Loss: -11122.932617\n",
      "Train Epoch: 33 [184320/663552 (28%)] Loss: -11122.889648\n",
      "Train Epoch: 33 [276480/663552 (42%)] Loss: -11122.975586\n",
      "Train Epoch: 33 [368640/663552 (56%)] Loss: -11122.857422\n",
      "Train Epoch: 33 [460800/663552 (69%)] Loss: -11122.871094\n",
      "Train Epoch: 33 [552960/663552 (83%)] Loss: -11122.991211\n",
      "Train Epoch: 33 [645120/663552 (97%)] Loss: -11122.839844\n",
      "    epoch          : 33\n",
      "    loss           : -11122.945713372877\n",
      "    val_loss       : -11122.95787217882\n",
      "Train Epoch: 34 [0/663552 (0%)] Loss: -11122.991211\n",
      "Train Epoch: 34 [92160/663552 (14%)] Loss: -11123.239258\n",
      "Train Epoch: 34 [184320/663552 (28%)] Loss: -11122.900391\n",
      "Train Epoch: 34 [276480/663552 (42%)] Loss: -11123.021484\n",
      "Train Epoch: 34 [368640/663552 (56%)] Loss: -11122.986328\n",
      "Train Epoch: 34 [460800/663552 (69%)] Loss: -11123.020508\n",
      "Train Epoch: 34 [552960/663552 (83%)] Loss: -11122.970703\n",
      "Train Epoch: 34 [645120/663552 (97%)] Loss: -11123.064453\n",
      "    epoch          : 34\n",
      "    loss           : -11122.946436752507\n",
      "    val_loss       : -11122.958306206598\n",
      "Train Epoch: 35 [0/663552 (0%)] Loss: -11123.056641\n",
      "Train Epoch: 35 [92160/663552 (14%)] Loss: -11123.010742\n",
      "Train Epoch: 35 [184320/663552 (28%)] Loss: -11123.049805\n",
      "Train Epoch: 35 [276480/663552 (42%)] Loss: -11122.962891\n",
      "Train Epoch: 35 [368640/663552 (56%)] Loss: -11122.708984\n",
      "Train Epoch: 35 [460800/663552 (69%)] Loss: -11122.923828\n",
      "Train Epoch: 35 [552960/663552 (83%)] Loss: -11122.919922\n",
      "Train Epoch: 35 [645120/663552 (97%)] Loss: -11123.022461\n",
      "    epoch          : 35\n",
      "    loss           : -11122.946919005593\n",
      "    val_loss       : -11122.95814344618\n",
      "Train Epoch: 36 [0/663552 (0%)] Loss: -11122.943359\n",
      "Train Epoch: 36 [92160/663552 (14%)] Loss: -11122.824219\n",
      "Train Epoch: 36 [184320/663552 (28%)] Loss: -11122.996094\n",
      "Train Epoch: 36 [276480/663552 (42%)] Loss: -11123.002930\n",
      "Train Epoch: 36 [368640/663552 (56%)] Loss: -11122.938477\n",
      "Train Epoch: 36 [460800/663552 (69%)] Loss: -11122.966797\n",
      "Train Epoch: 36 [552960/663552 (83%)] Loss: -11122.786133\n",
      "Train Epoch: 36 [645120/663552 (97%)] Loss: -11123.021484\n",
      "    epoch          : 36\n",
      "    loss           : -11122.947334948882\n",
      "    val_loss       : -11122.958902994791\n",
      "Train Epoch: 37 [0/663552 (0%)] Loss: -11123.000000\n",
      "Train Epoch: 37 [92160/663552 (14%)] Loss: -11123.041992\n",
      "Train Epoch: 37 [184320/663552 (28%)] Loss: -11122.795898\n",
      "Train Epoch: 37 [276480/663552 (42%)] Loss: -11122.857422\n",
      "Train Epoch: 37 [368640/663552 (56%)] Loss: -11122.862305\n",
      "Train Epoch: 37 [460800/663552 (69%)] Loss: -11123.000977\n",
      "Train Epoch: 37 [552960/663552 (83%)] Loss: -11122.903320\n",
      "Train Epoch: 37 [645120/663552 (97%)] Loss: -11122.990234\n",
      "    epoch          : 37\n",
      "    loss           : -11122.947859399113\n",
      "    val_loss       : -11122.959418402777\n",
      "Train Epoch: 38 [0/663552 (0%)] Loss: -11122.982422\n",
      "Train Epoch: 38 [92160/663552 (14%)] Loss: -11123.263672\n",
      "Train Epoch: 38 [184320/663552 (28%)] Loss: -11123.089844\n",
      "Train Epoch: 38 [276480/663552 (42%)] Loss: -11122.951172\n",
      "Train Epoch: 38 [368640/663552 (56%)] Loss: -11122.979492\n",
      "Train Epoch: 38 [460800/663552 (69%)] Loss: -11123.056641\n",
      "Train Epoch: 38 [552960/663552 (83%)] Loss: -11123.028320\n",
      "Train Epoch: 38 [645120/663552 (97%)] Loss: -11123.068359\n",
      "    epoch          : 38\n",
      "    loss           : -11122.94805531443\n",
      "    val_loss       : -11122.959526909723\n",
      "Train Epoch: 39 [0/663552 (0%)] Loss: -11123.002930\n",
      "Train Epoch: 39 [92160/663552 (14%)] Loss: -11122.942383\n",
      "Train Epoch: 39 [184320/663552 (28%)] Loss: -11122.924805\n",
      "Train Epoch: 39 [276480/663552 (42%)] Loss: -11122.898438\n",
      "Train Epoch: 39 [368640/663552 (56%)] Loss: -11122.948242\n",
      "Train Epoch: 39 [460800/663552 (69%)] Loss: -11122.840820\n",
      "Train Epoch: 39 [552960/663552 (83%)] Loss: -11122.907227\n",
      "Train Epoch: 39 [645120/663552 (97%)] Loss: -11123.100586\n",
      "    epoch          : 39\n",
      "    loss           : -11122.948456187307\n",
      "    val_loss       : -11122.960232204861\n",
      "Train Epoch: 40 [0/663552 (0%)] Loss: -11122.923828\n",
      "Train Epoch: 40 [92160/663552 (14%)] Loss: -11122.982422\n",
      "Train Epoch: 40 [184320/663552 (28%)] Loss: -11122.813477\n",
      "Train Epoch: 40 [276480/663552 (42%)] Loss: -11122.973633\n",
      "Train Epoch: 40 [368640/663552 (56%)] Loss: -11123.065430\n",
      "Train Epoch: 40 [460800/663552 (69%)] Loss: -11123.078125\n",
      "Train Epoch: 40 [552960/663552 (83%)] Loss: -11123.006836\n",
      "Train Epoch: 40 [645120/663552 (97%)] Loss: -11122.749023\n",
      "    epoch          : 40\n",
      "    loss           : -11122.948516468943\n",
      "    val_loss       : -11122.960286458334\n",
      "Saving checkpoint: saved/models/Shapes_ChyVAE/0802_175221/checkpoint-epoch40.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 41 [0/663552 (0%)] Loss: -11122.977539\n",
      "Train Epoch: 41 [92160/663552 (14%)] Loss: -11122.927734\n",
      "Train Epoch: 41 [184320/663552 (28%)] Loss: -11122.995117\n",
      "Train Epoch: 41 [276480/663552 (42%)] Loss: -11122.895508\n",
      "Train Epoch: 41 [368640/663552 (56%)] Loss: -11122.931641\n",
      "Train Epoch: 41 [460800/663552 (69%)] Loss: -11123.032227\n",
      "Train Epoch: 41 [552960/663552 (83%)] Loss: -11122.822266\n",
      "Train Epoch: 41 [645120/663552 (97%)] Loss: -11122.984375\n",
      "    epoch          : 41\n",
      "    loss           : -11122.948887201002\n",
      "    val_loss       : -11122.960720486111\n",
      "Train Epoch: 42 [0/663552 (0%)] Loss: -11123.098633\n",
      "Train Epoch: 42 [92160/663552 (14%)] Loss: -11122.932617\n",
      "Train Epoch: 42 [184320/663552 (28%)] Loss: -11123.034180\n",
      "Train Epoch: 42 [276480/663552 (42%)] Loss: -11123.003906\n",
      "Train Epoch: 42 [368640/663552 (56%)] Loss: -11123.029297\n",
      "Train Epoch: 42 [460800/663552 (69%)] Loss: -11123.057617\n",
      "Train Epoch: 42 [552960/663552 (83%)] Loss: -11122.968750\n",
      "Train Epoch: 42 [645120/663552 (97%)] Loss: -11122.832031\n",
      "    epoch          : 42\n",
      "    loss           : -11122.948890215084\n",
      "    val_loss       : -11122.960991753473\n",
      "Train Epoch: 43 [0/663552 (0%)] Loss: -11122.739258\n",
      "Train Epoch: 43 [92160/663552 (14%)] Loss: -11122.958008\n",
      "Train Epoch: 43 [184320/663552 (28%)] Loss: -11122.993164\n",
      "Train Epoch: 43 [276480/663552 (42%)] Loss: -11122.932617\n",
      "Train Epoch: 43 [368640/663552 (56%)] Loss: -11122.928711\n",
      "Train Epoch: 43 [460800/663552 (69%)] Loss: -11122.960938\n",
      "Train Epoch: 43 [552960/663552 (83%)] Loss: -11123.086914\n",
      "Train Epoch: 43 [645120/663552 (97%)] Loss: -11122.888672\n",
      "    epoch          : 43\n",
      "    loss           : -11122.949345341434\n",
      "    val_loss       : -11122.961100260416\n",
      "Train Epoch: 44 [0/663552 (0%)] Loss: -11122.841797\n",
      "Train Epoch: 44 [92160/663552 (14%)] Loss: -11122.778320\n",
      "Train Epoch: 44 [184320/663552 (28%)] Loss: -11123.077148\n",
      "Train Epoch: 44 [276480/663552 (42%)] Loss: -11123.157227\n",
      "Train Epoch: 44 [368640/663552 (56%)] Loss: -11122.907227\n",
      "Train Epoch: 44 [460800/663552 (69%)] Loss: -11122.928711\n",
      "Train Epoch: 44 [552960/663552 (83%)] Loss: -11122.823242\n",
      "Train Epoch: 44 [645120/663552 (97%)] Loss: -11122.969727\n",
      "    epoch          : 44\n",
      "    loss           : -11122.949390552661\n",
      "    val_loss       : -11122.960828993055\n",
      "Train Epoch: 45 [0/663552 (0%)] Loss: -11122.793945\n",
      "Train Epoch: 45 [92160/663552 (14%)] Loss: -11122.851562\n",
      "Train Epoch: 45 [184320/663552 (28%)] Loss: -11122.772461\n",
      "Train Epoch: 45 [276480/663552 (42%)] Loss: -11123.105469\n",
      "Train Epoch: 45 [368640/663552 (56%)] Loss: -11122.858398\n",
      "Train Epoch: 45 [460800/663552 (69%)] Loss: -11122.887695\n",
      "Train Epoch: 45 [552960/663552 (83%)] Loss: -11122.971680\n",
      "Train Epoch: 45 [645120/663552 (97%)] Loss: -11123.094727\n",
      "    epoch          : 45\n",
      "    loss           : -11122.949426721643\n",
      "    val_loss       : -11122.961018880209\n",
      "Train Epoch: 46 [0/663552 (0%)] Loss: -11122.749023\n",
      "Train Epoch: 46 [92160/663552 (14%)] Loss: -11122.904297\n",
      "Train Epoch: 46 [184320/663552 (28%)] Loss: -11122.920898\n",
      "Train Epoch: 46 [276480/663552 (42%)] Loss: -11122.879883\n",
      "Train Epoch: 46 [368640/663552 (56%)] Loss: -11122.869141\n",
      "Train Epoch: 46 [460800/663552 (69%)] Loss: -11122.852539\n",
      "Train Epoch: 46 [552960/663552 (83%)] Loss: -11122.793945\n",
      "Train Epoch: 46 [645120/663552 (97%)] Loss: -11122.828125\n",
      "    epoch          : 46\n",
      "    loss           : -11122.949634693286\n",
      "    val_loss       : -11122.960964626736\n",
      "Train Epoch: 47 [0/663552 (0%)] Loss: -11122.875000\n",
      "Train Epoch: 47 [92160/663552 (14%)] Loss: -11123.012695\n",
      "Train Epoch: 47 [184320/663552 (28%)] Loss: -11122.927734\n",
      "Train Epoch: 47 [276480/663552 (42%)] Loss: -11122.979492\n",
      "Train Epoch: 47 [368640/663552 (56%)] Loss: -11123.119141\n",
      "Train Epoch: 47 [460800/663552 (69%)] Loss: -11123.011719\n",
      "Train Epoch: 47 [552960/663552 (83%)] Loss: -11123.153320\n",
      "Train Epoch: 47 [645120/663552 (97%)] Loss: -11123.048828\n",
      "    epoch          : 47\n",
      "    loss           : -11122.949737172068\n",
      "    val_loss       : -11122.961046006945\n",
      "Train Epoch: 48 [0/663552 (0%)] Loss: -11122.960938\n",
      "Train Epoch: 48 [92160/663552 (14%)] Loss: -11123.007812\n",
      "Train Epoch: 48 [184320/663552 (28%)] Loss: -11122.773438\n",
      "Train Epoch: 48 [276480/663552 (42%)] Loss: -11122.992188\n",
      "Train Epoch: 48 [368640/663552 (56%)] Loss: -11122.860352\n",
      "Train Epoch: 48 [460800/663552 (69%)] Loss: -11122.953125\n",
      "Train Epoch: 48 [552960/663552 (83%)] Loss: -11122.894531\n",
      "Train Epoch: 48 [645120/663552 (97%)] Loss: -11122.770508\n",
      "    epoch          : 48\n",
      "    loss           : -11122.949782383295\n",
      "    val_loss       : -11122.961317274305\n",
      "Train Epoch: 49 [0/663552 (0%)] Loss: -11123.054688\n",
      "Train Epoch: 49 [92160/663552 (14%)] Loss: -11122.917969\n",
      "Train Epoch: 49 [184320/663552 (28%)] Loss: -11122.992188\n",
      "Train Epoch: 49 [276480/663552 (42%)] Loss: -11122.811523\n",
      "Train Epoch: 49 [368640/663552 (56%)] Loss: -11122.747070\n",
      "Train Epoch: 49 [460800/663552 (69%)] Loss: -11122.988281\n",
      "Train Epoch: 49 [552960/663552 (83%)] Loss: -11123.003906\n",
      "Train Epoch: 49 [645120/663552 (97%)] Loss: -11122.673828\n",
      "    epoch          : 49\n",
      "    loss           : -11122.949716073495\n",
      "    val_loss       : -11122.961941189236\n",
      "Train Epoch: 50 [0/663552 (0%)] Loss: -11122.746094\n",
      "Train Epoch: 50 [92160/663552 (14%)] Loss: -11123.101562\n",
      "Train Epoch: 50 [184320/663552 (28%)] Loss: -11122.988281\n",
      "Train Epoch: 50 [276480/663552 (42%)] Loss: -11122.953125\n",
      "Train Epoch: 50 [368640/663552 (56%)] Loss: -11122.771484\n",
      "Train Epoch: 50 [460800/663552 (69%)] Loss: -11122.939453\n",
      "Train Epoch: 50 [552960/663552 (83%)] Loss: -11122.933594\n",
      "Train Epoch: 50 [645120/663552 (97%)] Loss: -11122.591797\n",
      "    epoch          : 50\n",
      "    loss           : -11122.949957200039\n",
      "    val_loss       : -11122.961669921875\n",
      "Saving checkpoint: saved/models/Shapes_ChyVAE/0802_175221/checkpoint-epoch50.pth ...\n",
      "Train Epoch: 51 [0/663552 (0%)] Loss: -11123.120117\n",
      "Train Epoch: 51 [92160/663552 (14%)] Loss: -11122.786133\n",
      "Train Epoch: 51 [184320/663552 (28%)] Loss: -11122.935547\n",
      "Train Epoch: 51 [276480/663552 (42%)] Loss: -11122.874023\n",
      "Train Epoch: 51 [368640/663552 (56%)] Loss: -11122.983398\n",
      "Train Epoch: 51 [460800/663552 (69%)] Loss: -11122.866211\n",
      "Train Epoch: 51 [552960/663552 (83%)] Loss: -11122.990234\n",
      "Train Epoch: 51 [645120/663552 (97%)] Loss: -11122.799805\n",
      "    epoch          : 51\n",
      "    loss           : -11122.950264636382\n",
      "    val_loss       : -11122.961507161459\n",
      "Train Epoch: 52 [0/663552 (0%)] Loss: -11122.988281\n",
      "Train Epoch: 52 [92160/663552 (14%)] Loss: -11122.942383\n",
      "Train Epoch: 52 [184320/663552 (28%)] Loss: -11122.836914\n",
      "Train Epoch: 52 [276480/663552 (42%)] Loss: -11122.876953\n",
      "Train Epoch: 52 [368640/663552 (56%)] Loss: -11122.913086\n",
      "Train Epoch: 52 [460800/663552 (69%)] Loss: -11123.009766\n",
      "Train Epoch: 52 [552960/663552 (83%)] Loss: -11122.857422\n",
      "Train Epoch: 52 [645120/663552 (97%)] Loss: -11122.976562\n",
      "    epoch          : 52\n",
      "    loss           : -11122.950159143518\n",
      "    val_loss       : -11122.961859809027\n",
      "Train Epoch: 53 [0/663552 (0%)] Loss: -11123.073242\n",
      "Train Epoch: 53 [92160/663552 (14%)] Loss: -11122.898438\n",
      "Train Epoch: 53 [184320/663552 (28%)] Loss: -11122.926758\n",
      "Train Epoch: 53 [276480/663552 (42%)] Loss: -11122.934570\n",
      "Train Epoch: 53 [368640/663552 (56%)] Loss: -11123.092773\n",
      "Train Epoch: 53 [460800/663552 (69%)] Loss: -11122.986328\n",
      "Train Epoch: 53 [552960/663552 (83%)] Loss: -11123.021484\n",
      "Train Epoch: 53 [645120/663552 (97%)] Loss: -11122.838867\n",
      "    epoch          : 53\n",
      "    loss           : -11122.9500958478\n",
      "    val_loss       : -11122.961968315973\n",
      "Train Epoch: 54 [0/663552 (0%)] Loss: -11122.818359\n",
      "Train Epoch: 54 [92160/663552 (14%)] Loss: -11122.782227\n",
      "Train Epoch: 54 [184320/663552 (28%)] Loss: -11122.985352\n",
      "Train Epoch: 54 [276480/663552 (42%)] Loss: -11122.933594\n",
      "Train Epoch: 54 [368640/663552 (56%)] Loss: -11122.861328\n",
      "Train Epoch: 54 [460800/663552 (69%)] Loss: -11122.907227\n",
      "Train Epoch: 54 [552960/663552 (83%)] Loss: -11122.835938\n",
      "Train Epoch: 54 [645120/663552 (97%)] Loss: -11122.842773\n",
      "    epoch          : 54\n",
      "    loss           : -11122.950198326582\n",
      "    val_loss       : -11122.961886935764\n",
      "Train Epoch: 55 [0/663552 (0%)] Loss: -11122.976562\n",
      "Train Epoch: 55 [92160/663552 (14%)] Loss: -11122.974609\n",
      "Train Epoch: 55 [184320/663552 (28%)] Loss: -11122.916992\n",
      "Train Epoch: 55 [276480/663552 (42%)] Loss: -11123.031250\n",
      "Train Epoch: 55 [368640/663552 (56%)] Loss: -11122.961914\n",
      "Train Epoch: 55 [460800/663552 (69%)] Loss: -11122.894531\n",
      "Train Epoch: 55 [552960/663552 (83%)] Loss: -11122.964844\n",
      "Train Epoch: 55 [645120/663552 (97%)] Loss: -11122.980469\n",
      "    epoch          : 55\n",
      "    loss           : -11122.950370129243\n",
      "    val_loss       : -11122.962022569445\n",
      "Train Epoch: 56 [0/663552 (0%)] Loss: -11123.026367\n",
      "Train Epoch: 56 [92160/663552 (14%)] Loss: -11122.876953\n",
      "Train Epoch: 56 [184320/663552 (28%)] Loss: -11122.867188\n",
      "Train Epoch: 56 [276480/663552 (42%)] Loss: -11123.033203\n",
      "Train Epoch: 56 [368640/663552 (56%)] Loss: -11122.838867\n",
      "Train Epoch: 56 [460800/663552 (69%)] Loss: -11122.954102\n",
      "Train Epoch: 56 [552960/663552 (83%)] Loss: -11123.082031\n",
      "Train Epoch: 56 [645120/663552 (97%)] Loss: -11122.949219\n",
      "    epoch          : 56\n",
      "    loss           : -11122.950373143325\n",
      "    val_loss       : -11122.9619140625\n",
      "Train Epoch: 57 [0/663552 (0%)] Loss: -11122.920898\n",
      "Train Epoch: 57 [92160/663552 (14%)] Loss: -11123.145508\n",
      "Train Epoch: 57 [184320/663552 (28%)] Loss: -11123.001953\n",
      "Train Epoch: 57 [276480/663552 (42%)] Loss: -11123.086914\n",
      "Train Epoch: 57 [368640/663552 (56%)] Loss: -11122.938477\n",
      "Train Epoch: 57 [460800/663552 (69%)] Loss: -11122.957031\n",
      "Train Epoch: 57 [552960/663552 (83%)] Loss: -11123.056641\n",
      "Train Epoch: 57 [645120/663552 (97%)] Loss: -11123.000000\n",
      "    epoch          : 57\n",
      "    loss           : -11122.950346016589\n",
      "    val_loss       : -11122.96177842882\n",
      "Train Epoch: 58 [0/663552 (0%)] Loss: -11122.847656\n",
      "Train Epoch: 58 [92160/663552 (14%)] Loss: -11122.984375\n",
      "Train Epoch: 58 [184320/663552 (28%)] Loss: -11122.937500\n",
      "Train Epoch: 58 [276480/663552 (42%)] Loss: -11122.860352\n",
      "Train Epoch: 58 [368640/663552 (56%)] Loss: -11123.036133\n",
      "Train Epoch: 58 [460800/663552 (69%)] Loss: -11122.946289\n",
      "Train Epoch: 58 [552960/663552 (83%)] Loss: -11123.005859\n",
      "Train Epoch: 58 [645120/663552 (97%)] Loss: -11122.984375\n",
      "    epoch          : 58\n",
      "    loss           : -11122.950563030477\n",
      "    val_loss       : -11122.961588541666\n",
      "Train Epoch: 59 [0/663552 (0%)] Loss: -11122.875000\n",
      "Train Epoch: 59 [92160/663552 (14%)] Loss: -11123.059570\n",
      "Train Epoch: 59 [184320/663552 (28%)] Loss: -11122.834961\n",
      "Train Epoch: 59 [276480/663552 (42%)] Loss: -11122.946289\n",
      "Train Epoch: 59 [368640/663552 (56%)] Loss: -11122.996094\n",
      "Train Epoch: 59 [460800/663552 (69%)] Loss: -11122.889648\n",
      "Train Epoch: 59 [552960/663552 (83%)] Loss: -11122.783203\n",
      "Train Epoch: 59 [645120/663552 (97%)] Loss: -11123.039062\n",
      "    epoch          : 59\n",
      "    loss           : -11122.950222439236\n",
      "    val_loss       : -11122.961941189236\n",
      "Train Epoch: 60 [0/663552 (0%)] Loss: -11122.802734\n",
      "Train Epoch: 60 [92160/663552 (14%)] Loss: -11122.955078\n",
      "Train Epoch: 60 [184320/663552 (28%)] Loss: -11122.844727\n",
      "Train Epoch: 60 [276480/663552 (42%)] Loss: -11122.886719\n",
      "Train Epoch: 60 [368640/663552 (56%)] Loss: -11122.805664\n",
      "Train Epoch: 60 [460800/663552 (69%)] Loss: -11122.791992\n",
      "Train Epoch: 60 [552960/663552 (83%)] Loss: -11122.751953\n",
      "Train Epoch: 60 [645120/663552 (97%)] Loss: -11122.761719\n",
      "    epoch          : 60\n",
      "    loss           : -11122.950361086998\n",
      "    val_loss       : -11122.961724175348\n",
      "Saving checkpoint: saved/models/Shapes_ChyVAE/0802_175221/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [0/663552 (0%)] Loss: -11122.868164\n",
      "Train Epoch: 61 [92160/663552 (14%)] Loss: -11122.769531\n",
      "Train Epoch: 61 [184320/663552 (28%)] Loss: -11122.901367\n",
      "Train Epoch: 61 [276480/663552 (42%)] Loss: -11123.068359\n",
      "Train Epoch: 61 [368640/663552 (56%)] Loss: -11122.840820\n",
      "Train Epoch: 61 [460800/663552 (69%)] Loss: -11122.805664\n",
      "Train Epoch: 61 [552960/663552 (83%)] Loss: -11122.913086\n",
      "Train Epoch: 61 [645120/663552 (97%)] Loss: -11122.982422\n",
      "    epoch          : 61\n",
      "    loss           : -11122.950541931907\n",
      "    val_loss       : -11122.961805555555\n",
      "Train Epoch: 62 [0/663552 (0%)] Loss: -11122.876953\n",
      "Train Epoch: 62 [92160/663552 (14%)] Loss: -11122.862305\n",
      "Train Epoch: 62 [184320/663552 (28%)] Loss: -11123.042969\n",
      "Train Epoch: 62 [276480/663552 (42%)] Loss: -11123.025391\n",
      "Train Epoch: 62 [368640/663552 (56%)] Loss: -11122.635742\n",
      "Train Epoch: 62 [460800/663552 (69%)] Loss: -11122.808594\n",
      "Train Epoch: 62 [552960/663552 (83%)] Loss: -11122.970703\n",
      "Train Epoch: 62 [645120/663552 (97%)] Loss: -11122.864258\n",
      "    epoch          : 62\n",
      "    loss           : -11122.950544945988\n",
      "    val_loss       : -11122.962103949652\n",
      "Train Epoch: 63 [0/663552 (0%)] Loss: -11122.921875\n",
      "Train Epoch: 63 [92160/663552 (14%)] Loss: -11122.888672\n",
      "Train Epoch: 63 [184320/663552 (28%)] Loss: -11123.025391\n",
      "Train Epoch: 63 [276480/663552 (42%)] Loss: -11122.864258\n",
      "Train Epoch: 63 [368640/663552 (56%)] Loss: -11123.109375\n",
      "Train Epoch: 63 [460800/663552 (69%)] Loss: -11122.996094\n",
      "Train Epoch: 63 [552960/663552 (83%)] Loss: -11123.214844\n",
      "Train Epoch: 63 [645120/663552 (97%)] Loss: -11122.913086\n",
      "    epoch          : 63\n",
      "    loss           : -11122.950526861498\n",
      "    val_loss       : -11122.962293836805\n",
      "Train Epoch: 64 [0/663552 (0%)] Loss: -11123.008789\n",
      "Train Epoch: 64 [92160/663552 (14%)] Loss: -11122.922852\n",
      "Train Epoch: 64 [184320/663552 (28%)] Loss: -11122.930664\n",
      "Train Epoch: 64 [276480/663552 (42%)] Loss: -11123.067383\n",
      "Train Epoch: 64 [368640/663552 (56%)] Loss: -11122.844727\n",
      "Train Epoch: 64 [460800/663552 (69%)] Loss: -11123.012695\n",
      "Train Epoch: 64 [552960/663552 (83%)] Loss: -11122.894531\n",
      "Train Epoch: 64 [645120/663552 (97%)] Loss: -11122.852539\n",
      "    epoch          : 64\n",
      "    loss           : -11122.950575086805\n",
      "    val_loss       : -11122.962348090277\n",
      "Train Epoch: 65 [0/663552 (0%)] Loss: -11123.116211\n",
      "Train Epoch: 65 [92160/663552 (14%)] Loss: -11122.875000\n",
      "Train Epoch: 65 [184320/663552 (28%)] Loss: -11123.032227\n",
      "Train Epoch: 65 [276480/663552 (42%)] Loss: -11122.956055\n",
      "Train Epoch: 65 [368640/663552 (56%)] Loss: -11122.893555\n",
      "Train Epoch: 65 [460800/663552 (69%)] Loss: -11122.886719\n",
      "Train Epoch: 65 [552960/663552 (83%)] Loss: -11122.996094\n",
      "Train Epoch: 65 [645120/663552 (97%)] Loss: -11122.980469\n",
      "    epoch          : 65\n",
      "    loss           : -11122.95054796007\n",
      "    val_loss       : -11122.962076822916\n",
      "Train Epoch: 66 [0/663552 (0%)] Loss: -11123.089844\n",
      "Train Epoch: 66 [92160/663552 (14%)] Loss: -11122.846680\n",
      "Train Epoch: 66 [184320/663552 (28%)] Loss: -11122.849609\n",
      "Train Epoch: 66 [276480/663552 (42%)] Loss: -11122.759766\n",
      "Train Epoch: 66 [368640/663552 (56%)] Loss: -11122.885742\n",
      "Train Epoch: 66 [460800/663552 (69%)] Loss: -11122.878906\n",
      "Train Epoch: 66 [552960/663552 (83%)] Loss: -11123.096680\n",
      "Train Epoch: 66 [645120/663552 (97%)] Loss: -11122.877930\n",
      "    epoch          : 66\n",
      "    loss           : -11122.950647424768\n",
      "    val_loss       : -11122.962076822916\n",
      "Train Epoch: 67 [0/663552 (0%)] Loss: -11122.925781\n",
      "Train Epoch: 67 [92160/663552 (14%)] Loss: -11122.984375\n",
      "Train Epoch: 67 [184320/663552 (28%)] Loss: -11122.809570\n",
      "Train Epoch: 67 [276480/663552 (42%)] Loss: -11123.055664\n",
      "Train Epoch: 67 [368640/663552 (56%)] Loss: -11122.888672\n",
      "Train Epoch: 67 [460800/663552 (69%)] Loss: -11123.103516\n",
      "Train Epoch: 67 [552960/663552 (83%)] Loss: -11123.113281\n",
      "Train Epoch: 67 [645120/663552 (97%)] Loss: -11122.701172\n",
      "    epoch          : 67\n",
      "    loss           : -11122.950644410686\n",
      "    val_loss       : -11122.962076822916\n",
      "Train Epoch: 68 [0/663552 (0%)] Loss: -11122.995117\n",
      "Train Epoch: 68 [92160/663552 (14%)] Loss: -11123.104492\n",
      "Train Epoch: 68 [184320/663552 (28%)] Loss: -11122.928711\n",
      "Train Epoch: 68 [276480/663552 (42%)] Loss: -11122.889648\n",
      "Train Epoch: 68 [368640/663552 (56%)] Loss: -11123.055664\n",
      "Train Epoch: 68 [460800/663552 (69%)] Loss: -11123.028320\n",
      "Train Epoch: 68 [552960/663552 (83%)] Loss: -11122.835938\n",
      "Train Epoch: 68 [645120/663552 (97%)] Loss: -11123.031250\n",
      "    epoch          : 68\n",
      "    loss           : -11122.950710720486\n",
      "    val_loss       : -11122.961995442709\n",
      "Train Epoch: 69 [0/663552 (0%)] Loss: -11123.007812\n",
      "Train Epoch: 69 [92160/663552 (14%)] Loss: -11122.969727\n",
      "Train Epoch: 69 [184320/663552 (28%)] Loss: -11122.802734\n",
      "Train Epoch: 69 [276480/663552 (42%)] Loss: -11123.093750\n",
      "Train Epoch: 69 [368640/663552 (56%)] Loss: -11122.849609\n",
      "Train Epoch: 69 [460800/663552 (69%)] Loss: -11123.068359\n",
      "Train Epoch: 69 [552960/663552 (83%)] Loss: -11122.965820\n",
      "Train Epoch: 69 [645120/663552 (97%)] Loss: -11122.981445\n",
      "    epoch          : 69\n",
      "    loss           : -11122.950728804977\n",
      "    val_loss       : -11122.96253797743\n",
      "Train Epoch: 70 [0/663552 (0%)] Loss: -11122.917969\n",
      "Train Epoch: 70 [92160/663552 (14%)] Loss: -11122.954102\n",
      "Train Epoch: 70 [184320/663552 (28%)] Loss: -11122.888672\n",
      "Train Epoch: 70 [276480/663552 (42%)] Loss: -11122.954102\n",
      "Train Epoch: 70 [368640/663552 (56%)] Loss: -11122.829102\n",
      "Train Epoch: 70 [460800/663552 (69%)] Loss: -11122.976562\n",
      "Train Epoch: 70 [552960/663552 (83%)] Loss: -11123.176758\n",
      "Train Epoch: 70 [645120/663552 (97%)] Loss: -11122.972656\n",
      "    epoch          : 70\n",
      "    loss           : -11122.95071674865\n",
      "    val_loss       : -11122.962375217014\n",
      "Saving checkpoint: saved/models/Shapes_ChyVAE/0802_175221/checkpoint-epoch70.pth ...\n",
      "Train Epoch: 71 [0/663552 (0%)] Loss: -11122.909180\n",
      "Train Epoch: 71 [92160/663552 (14%)] Loss: -11123.252930\n",
      "Train Epoch: 71 [184320/663552 (28%)] Loss: -11122.970703\n",
      "Train Epoch: 71 [276480/663552 (42%)] Loss: -11122.998047\n",
      "Train Epoch: 71 [368640/663552 (56%)] Loss: -11122.780273\n",
      "Train Epoch: 71 [460800/663552 (69%)] Loss: -11122.943359\n",
      "Train Epoch: 71 [552960/663552 (83%)] Loss: -11122.892578\n",
      "Train Epoch: 71 [645120/663552 (97%)] Loss: -11122.889648\n",
      "    epoch          : 71\n",
      "    loss           : -11122.950804157023\n",
      "    val_loss       : -11122.962619357639\n",
      "Train Epoch: 72 [0/663552 (0%)] Loss: -11123.135742\n",
      "Train Epoch: 72 [92160/663552 (14%)] Loss: -11123.067383\n",
      "Train Epoch: 72 [184320/663552 (28%)] Loss: -11122.875000\n",
      "Train Epoch: 72 [276480/663552 (42%)] Loss: -11123.024414\n",
      "Train Epoch: 72 [368640/663552 (56%)] Loss: -11122.857422\n",
      "Train Epoch: 72 [460800/663552 (69%)] Loss: -11123.012695\n",
      "Train Epoch: 72 [552960/663552 (83%)] Loss: -11122.976562\n",
      "Train Epoch: 72 [645120/663552 (97%)] Loss: -11123.203125\n",
      "    epoch          : 72\n",
      "    loss           : -11122.950713734568\n",
      "    val_loss       : -11122.96240234375\n",
      "Train Epoch: 73 [0/663552 (0%)] Loss: -11122.862305\n",
      "Train Epoch: 73 [92160/663552 (14%)] Loss: -11122.982422\n",
      "Train Epoch: 73 [184320/663552 (28%)] Loss: -11122.964844\n",
      "Train Epoch: 73 [276480/663552 (42%)] Loss: -11122.852539\n",
      "Train Epoch: 73 [368640/663552 (56%)] Loss: -11122.920898\n",
      "Train Epoch: 73 [460800/663552 (69%)] Loss: -11123.022461\n",
      "Train Epoch: 73 [552960/663552 (83%)] Loss: -11122.965820\n",
      "Train Epoch: 73 [645120/663552 (97%)] Loss: -11122.824219\n",
      "    epoch          : 73\n",
      "    loss           : -11122.95071674865\n",
      "    val_loss       : -11122.962646484375\n",
      "Train Epoch: 74 [0/663552 (0%)] Loss: -11122.964844\n",
      "Train Epoch: 74 [92160/663552 (14%)] Loss: -11123.047852\n",
      "Train Epoch: 74 [184320/663552 (28%)] Loss: -11122.873047\n",
      "Train Epoch: 74 [276480/663552 (42%)] Loss: -11122.822266\n",
      "Train Epoch: 74 [368640/663552 (56%)] Loss: -11122.964844\n",
      "Train Epoch: 74 [460800/663552 (69%)] Loss: -11122.788086\n",
      "Train Epoch: 74 [552960/663552 (83%)] Loss: -11122.887695\n",
      "Train Epoch: 74 [645120/663552 (97%)] Loss: -11123.040039\n",
      "    epoch          : 74\n",
      "    loss           : -11122.950737847223\n",
      "    val_loss       : -11122.962103949652\n",
      "Train Epoch: 75 [0/663552 (0%)] Loss: -11122.906250\n",
      "Train Epoch: 75 [92160/663552 (14%)] Loss: -11122.957031\n",
      "Train Epoch: 75 [184320/663552 (28%)] Loss: -11122.937500\n",
      "Train Epoch: 75 [276480/663552 (42%)] Loss: -11123.064453\n",
      "Train Epoch: 75 [368640/663552 (56%)] Loss: -11122.796875\n",
      "Train Epoch: 75 [460800/663552 (69%)] Loss: -11123.116211\n",
      "Train Epoch: 75 [552960/663552 (83%)] Loss: -11122.855469\n",
      "Train Epoch: 75 [645120/663552 (97%)] Loss: -11122.916016\n",
      "    epoch          : 75\n",
      "    loss           : -11122.95073483314\n",
      "    val_loss       : -11122.962293836805\n",
      "Train Epoch: 76 [0/663552 (0%)] Loss: -11123.040039\n",
      "Train Epoch: 76 [92160/663552 (14%)] Loss: -11123.019531\n",
      "Train Epoch: 76 [184320/663552 (28%)] Loss: -11122.962891\n",
      "Train Epoch: 76 [276480/663552 (42%)] Loss: -11122.963867\n",
      "Train Epoch: 76 [368640/663552 (56%)] Loss: -11122.938477\n",
      "Train Epoch: 76 [460800/663552 (69%)] Loss: -11122.805664\n",
      "Train Epoch: 76 [552960/663552 (83%)] Loss: -11123.038086\n",
      "Train Epoch: 76 [645120/663552 (97%)] Loss: -11122.871094\n",
      "    epoch          : 76\n",
      "    loss           : -11122.950828269675\n",
      "    val_loss       : -11122.962293836805\n",
      "Train Epoch: 77 [0/663552 (0%)] Loss: -11123.106445\n",
      "Train Epoch: 77 [92160/663552 (14%)] Loss: -11122.875000\n",
      "Train Epoch: 77 [184320/663552 (28%)] Loss: -11122.992188\n",
      "Train Epoch: 77 [276480/663552 (42%)] Loss: -11122.739258\n",
      "Train Epoch: 77 [368640/663552 (56%)] Loss: -11123.023438\n",
      "Train Epoch: 77 [460800/663552 (69%)] Loss: -11122.999023\n",
      "Train Epoch: 77 [552960/663552 (83%)] Loss: -11123.003906\n",
      "Train Epoch: 77 [645120/663552 (97%)] Loss: -11123.108398\n",
      "    epoch          : 77\n",
      "    loss           : -11122.95074990355\n",
      "    val_loss       : -11122.962375217014\n",
      "Train Epoch: 78 [0/663552 (0%)] Loss: -11123.153320\n",
      "Train Epoch: 78 [92160/663552 (14%)] Loss: -11122.996094\n",
      "Train Epoch: 78 [184320/663552 (28%)] Loss: -11123.061523\n",
      "Train Epoch: 78 [276480/663552 (42%)] Loss: -11123.015625\n",
      "Train Epoch: 78 [368640/663552 (56%)] Loss: -11122.987305\n",
      "Train Epoch: 78 [460800/663552 (69%)] Loss: -11123.003906\n",
      "Train Epoch: 78 [552960/663552 (83%)] Loss: -11122.872070\n",
      "Train Epoch: 78 [645120/663552 (97%)] Loss: -11123.021484\n",
      "    epoch          : 78\n",
      "    loss           : -11122.950774016204\n",
      "    val_loss       : -11122.962293836805\n",
      "Train Epoch: 79 [0/663552 (0%)] Loss: -11122.912109\n",
      "Train Epoch: 79 [92160/663552 (14%)] Loss: -11123.118164\n",
      "Train Epoch: 79 [184320/663552 (28%)] Loss: -11122.837891\n",
      "Train Epoch: 79 [276480/663552 (42%)] Loss: -11122.899414\n",
      "Train Epoch: 79 [368640/663552 (56%)] Loss: -11123.097656\n",
      "Train Epoch: 79 [460800/663552 (69%)] Loss: -11122.915039\n",
      "Train Epoch: 79 [552960/663552 (83%)] Loss: -11122.920898\n",
      "Train Epoch: 79 [645120/663552 (97%)] Loss: -11122.968750\n",
      "    epoch          : 79\n",
      "    loss           : -11122.950855396411\n",
      "    val_loss       : -11122.962239583334\n",
      "Train Epoch: 80 [0/663552 (0%)] Loss: -11122.928711\n",
      "Train Epoch: 80 [92160/663552 (14%)] Loss: -11123.033203\n",
      "Train Epoch: 80 [184320/663552 (28%)] Loss: -11122.801758\n",
      "Train Epoch: 80 [276480/663552 (42%)] Loss: -11123.090820\n",
      "Train Epoch: 80 [368640/663552 (56%)] Loss: -11122.922852\n",
      "Train Epoch: 80 [460800/663552 (69%)] Loss: -11122.927734\n",
      "Train Epoch: 80 [552960/663552 (83%)] Loss: -11122.977539\n",
      "Train Epoch: 80 [645120/663552 (97%)] Loss: -11122.966797\n",
      "    epoch          : 80\n",
      "    loss           : -11122.95068359375\n",
      "    val_loss       : -11122.96240234375\n",
      "Saving checkpoint: saved/models/Shapes_ChyVAE/0802_175221/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [0/663552 (0%)] Loss: -11122.786133\n",
      "Train Epoch: 81 [92160/663552 (14%)] Loss: -11122.857422\n",
      "Train Epoch: 81 [184320/663552 (28%)] Loss: -11122.847656\n",
      "Train Epoch: 81 [276480/663552 (42%)] Loss: -11123.197266\n",
      "Train Epoch: 81 [368640/663552 (56%)] Loss: -11122.955078\n",
      "Train Epoch: 81 [460800/663552 (69%)] Loss: -11122.987305\n",
      "Train Epoch: 81 [552960/663552 (83%)] Loss: -11122.910156\n",
      "Train Epoch: 81 [645120/663552 (97%)] Loss: -11123.092773\n",
      "    epoch          : 81\n",
      "    loss           : -11122.950849368248\n",
      "    val_loss       : -11122.962212456598\n",
      "Train Epoch: 82 [0/663552 (0%)] Loss: -11123.021484\n",
      "Train Epoch: 82 [92160/663552 (14%)] Loss: -11123.035156\n",
      "Train Epoch: 82 [184320/663552 (28%)] Loss: -11122.854492\n",
      "Train Epoch: 82 [276480/663552 (42%)] Loss: -11123.107422\n",
      "Train Epoch: 82 [368640/663552 (56%)] Loss: -11122.873047\n",
      "Train Epoch: 82 [460800/663552 (69%)] Loss: -11122.916992\n",
      "Train Epoch: 82 [552960/663552 (83%)] Loss: -11122.882812\n",
      "Train Epoch: 82 [645120/663552 (97%)] Loss: -11122.985352\n",
      "    epoch          : 82\n",
      "    loss           : -11122.950816213348\n",
      "    val_loss       : -11122.962348090277\n",
      "Train Epoch: 83 [0/663552 (0%)] Loss: -11123.024414\n",
      "Train Epoch: 83 [92160/663552 (14%)] Loss: -11123.048828\n",
      "Train Epoch: 83 [184320/663552 (28%)] Loss: -11123.115234\n",
      "Train Epoch: 83 [276480/663552 (42%)] Loss: -11122.944336\n",
      "Train Epoch: 83 [368640/663552 (56%)] Loss: -11122.984375\n",
      "Train Epoch: 83 [460800/663552 (69%)] Loss: -11122.911133\n",
      "Train Epoch: 83 [552960/663552 (83%)] Loss: -11122.977539\n",
      "Train Epoch: 83 [645120/663552 (97%)] Loss: -11123.104492\n",
      "    epoch          : 83\n",
      "    loss           : -11122.950849368248\n",
      "    val_loss       : -11122.962429470486\n",
      "Train Epoch: 84 [0/663552 (0%)] Loss: -11122.890625\n",
      "Train Epoch: 84 [92160/663552 (14%)] Loss: -11122.939453\n",
      "Train Epoch: 84 [184320/663552 (28%)] Loss: -11123.009766\n",
      "Train Epoch: 84 [276480/663552 (42%)] Loss: -11122.968750\n",
      "Train Epoch: 84 [368640/663552 (56%)] Loss: -11123.194336\n",
      "Train Epoch: 84 [460800/663552 (69%)] Loss: -11123.162109\n",
      "Train Epoch: 84 [552960/663552 (83%)] Loss: -11122.868164\n",
      "Train Epoch: 84 [645120/663552 (97%)] Loss: -11123.111328\n",
      "    epoch          : 84\n",
      "    loss           : -11122.950813199266\n",
      "    val_loss       : -11122.962375217014\n",
      "Train Epoch: 85 [0/663552 (0%)] Loss: -11122.980469\n",
      "Train Epoch: 85 [92160/663552 (14%)] Loss: -11122.608398\n",
      "Train Epoch: 85 [184320/663552 (28%)] Loss: -11122.813477\n",
      "Train Epoch: 85 [276480/663552 (42%)] Loss: -11122.883789\n",
      "Train Epoch: 85 [368640/663552 (56%)] Loss: -11122.992188\n",
      "Train Epoch: 85 [460800/663552 (69%)] Loss: -11123.032227\n",
      "Train Epoch: 85 [552960/663552 (83%)] Loss: -11122.747070\n",
      "Train Epoch: 85 [645120/663552 (97%)] Loss: -11123.020508\n",
      "    epoch          : 85\n",
      "    loss           : -11122.950888551311\n",
      "    val_loss       : -11122.962510850695\n",
      "Train Epoch: 86 [0/663552 (0%)] Loss: -11122.872070\n",
      "Train Epoch: 86 [92160/663552 (14%)] Loss: -11123.103516\n",
      "Train Epoch: 86 [184320/663552 (28%)] Loss: -11123.020508\n",
      "Train Epoch: 86 [276480/663552 (42%)] Loss: -11122.777344\n",
      "Train Epoch: 86 [368640/663552 (56%)] Loss: -11122.971680\n",
      "Train Epoch: 86 [460800/663552 (69%)] Loss: -11122.837891\n",
      "Train Epoch: 86 [552960/663552 (83%)] Loss: -11123.089844\n",
      "Train Epoch: 86 [645120/663552 (97%)] Loss: -11122.964844\n",
      "    epoch          : 86\n",
      "    loss           : -11122.950894579475\n",
      "    val_loss       : -11122.962131076389\n",
      "Train Epoch: 87 [0/663552 (0%)] Loss: -11123.032227\n",
      "Train Epoch: 87 [92160/663552 (14%)] Loss: -11122.833984\n",
      "Train Epoch: 87 [184320/663552 (28%)] Loss: -11122.801758\n",
      "Train Epoch: 87 [276480/663552 (42%)] Loss: -11123.129883\n",
      "Train Epoch: 87 [368640/663552 (56%)] Loss: -11122.905273\n",
      "Train Epoch: 87 [460800/663552 (69%)] Loss: -11122.946289\n",
      "Train Epoch: 87 [552960/663552 (83%)] Loss: -11123.140625\n",
      "Train Epoch: 87 [645120/663552 (97%)] Loss: -11123.041992\n",
      "    epoch          : 87\n",
      "    loss           : -11122.950972945602\n",
      "    val_loss       : -11122.962456597223\n",
      "Train Epoch: 88 [0/663552 (0%)] Loss: -11122.849609\n",
      "Train Epoch: 88 [92160/663552 (14%)] Loss: -11122.775391\n",
      "Train Epoch: 88 [184320/663552 (28%)] Loss: -11122.849609\n",
      "Train Epoch: 88 [276480/663552 (42%)] Loss: -11122.882812\n",
      "Train Epoch: 88 [368640/663552 (56%)] Loss: -11123.027344\n",
      "Train Epoch: 88 [460800/663552 (69%)] Loss: -11122.991211\n",
      "Train Epoch: 88 [552960/663552 (83%)] Loss: -11123.150391\n",
      "Train Epoch: 88 [645120/663552 (97%)] Loss: -11123.136719\n",
      "    epoch          : 88\n",
      "    loss           : -11122.950960889275\n",
      "    val_loss       : -11122.962619357639\n",
      "Train Epoch: 89 [0/663552 (0%)] Loss: -11122.983398\n",
      "Train Epoch: 89 [92160/663552 (14%)] Loss: -11122.892578\n",
      "Train Epoch: 89 [184320/663552 (28%)] Loss: -11122.842773\n",
      "Train Epoch: 89 [276480/663552 (42%)] Loss: -11122.864258\n",
      "Train Epoch: 89 [368640/663552 (56%)] Loss: -11123.045898\n",
      "Train Epoch: 89 [460800/663552 (69%)] Loss: -11122.941406\n",
      "Train Epoch: 89 [552960/663552 (83%)] Loss: -11122.873047\n",
      "Train Epoch: 89 [645120/663552 (97%)] Loss: -11122.952148\n",
      "    epoch          : 89\n",
      "    loss           : -11122.950945818866\n",
      "    val_loss       : -11122.96240234375\n",
      "Train Epoch: 90 [0/663552 (0%)] Loss: -11122.760742\n",
      "Train Epoch: 90 [92160/663552 (14%)] Loss: -11122.773438\n",
      "Train Epoch: 90 [184320/663552 (28%)] Loss: -11122.945312\n",
      "Train Epoch: 90 [276480/663552 (42%)] Loss: -11122.932617\n",
      "Train Epoch: 90 [368640/663552 (56%)] Loss: -11123.065430\n",
      "Train Epoch: 90 [460800/663552 (69%)] Loss: -11122.986328\n",
      "Train Epoch: 90 [552960/663552 (83%)] Loss: -11122.848633\n",
      "Train Epoch: 90 [645120/663552 (97%)] Loss: -11122.795898\n",
      "    epoch          : 90\n",
      "    loss           : -11122.95098500193\n",
      "    val_loss       : -11122.962646484375\n",
      "Saving checkpoint: saved/models/Shapes_ChyVAE/0802_175221/checkpoint-epoch90.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 91 [0/663552 (0%)] Loss: -11122.964844\n",
      "Train Epoch: 91 [92160/663552 (14%)] Loss: -11122.810547\n",
      "Train Epoch: 91 [184320/663552 (28%)] Loss: -11122.842773\n",
      "Train Epoch: 91 [276480/663552 (42%)] Loss: -11122.851562\n",
      "Train Epoch: 91 [368640/663552 (56%)] Loss: -11122.904297\n",
      "Train Epoch: 91 [460800/663552 (69%)] Loss: -11123.235352\n",
      "Train Epoch: 91 [552960/663552 (83%)] Loss: -11122.819336\n",
      "Train Epoch: 91 [645120/663552 (97%)] Loss: -11122.642578\n",
      "    epoch          : 91\n",
      "    loss           : -11122.95091869213\n",
      "    val_loss       : -11122.96253797743\n",
      "Train Epoch: 92 [0/663552 (0%)] Loss: -11122.951172\n",
      "Train Epoch: 92 [92160/663552 (14%)] Loss: -11123.049805\n",
      "Train Epoch: 92 [184320/663552 (28%)] Loss: -11122.872070\n",
      "Train Epoch: 92 [276480/663552 (42%)] Loss: -11123.085938\n",
      "Train Epoch: 92 [368640/663552 (56%)] Loss: -11123.149414\n",
      "Train Epoch: 92 [460800/663552 (69%)] Loss: -11122.920898\n",
      "Train Epoch: 92 [552960/663552 (83%)] Loss: -11123.025391\n",
      "Train Epoch: 92 [645120/663552 (97%)] Loss: -11122.729492\n",
      "    epoch          : 92\n",
      "    loss           : -11122.951000072339\n",
      "    val_loss       : -11122.962375217014\n",
      "Train Epoch: 93 [0/663552 (0%)] Loss: -11122.916992\n",
      "Train Epoch: 93 [92160/663552 (14%)] Loss: -11123.020508\n",
      "Train Epoch: 93 [184320/663552 (28%)] Loss: -11122.943359\n",
      "Train Epoch: 93 [276480/663552 (42%)] Loss: -11122.753906\n",
      "Train Epoch: 93 [368640/663552 (56%)] Loss: -11122.958984\n",
      "Train Epoch: 93 [460800/663552 (69%)] Loss: -11122.828125\n",
      "Train Epoch: 93 [552960/663552 (83%)] Loss: -11122.971680\n",
      "Train Epoch: 93 [645120/663552 (97%)] Loss: -11122.887695\n",
      "    epoch          : 93\n",
      "    loss           : -11122.95096993152\n",
      "    val_loss       : -11122.962673611111\n",
      "Train Epoch: 94 [0/663552 (0%)] Loss: -11122.829102\n",
      "Train Epoch: 94 [92160/663552 (14%)] Loss: -11122.846680\n",
      "Train Epoch: 94 [184320/663552 (28%)] Loss: -11122.846680\n",
      "Train Epoch: 94 [276480/663552 (42%)] Loss: -11122.971680\n",
      "Train Epoch: 94 [368640/663552 (56%)] Loss: -11122.837891\n",
      "Train Epoch: 94 [460800/663552 (69%)] Loss: -11123.242188\n",
      "Train Epoch: 94 [552960/663552 (83%)] Loss: -11122.978516\n",
      "Train Epoch: 94 [645120/663552 (97%)] Loss: -11123.106445\n",
      "    epoch          : 94\n",
      "    loss           : -11122.951039255402\n",
      "    val_loss       : -11122.962565104166\n",
      "Train Epoch: 95 [0/663552 (0%)] Loss: -11123.020508\n",
      "Train Epoch: 95 [92160/663552 (14%)] Loss: -11122.750977\n",
      "Train Epoch: 95 [184320/663552 (28%)] Loss: -11122.848633\n",
      "Train Epoch: 95 [276480/663552 (42%)] Loss: -11122.951172\n",
      "Train Epoch: 95 [368640/663552 (56%)] Loss: -11122.800781\n",
      "Train Epoch: 95 [460800/663552 (69%)] Loss: -11122.827148\n",
      "Train Epoch: 95 [552960/663552 (83%)] Loss: -11122.974609\n",
      "Train Epoch: 95 [645120/663552 (97%)] Loss: -11122.901367\n",
      "    epoch          : 95\n",
      "    loss           : -11122.950945818866\n",
      "    val_loss       : -11122.962782118055\n",
      "Train Epoch: 96 [0/663552 (0%)] Loss: -11123.116211\n",
      "Train Epoch: 96 [92160/663552 (14%)] Loss: -11123.298828\n",
      "Train Epoch: 96 [184320/663552 (28%)] Loss: -11123.130859\n",
      "Train Epoch: 96 [276480/663552 (42%)] Loss: -11122.765625\n",
      "Train Epoch: 96 [368640/663552 (56%)] Loss: -11123.000977\n",
      "Train Epoch: 96 [460800/663552 (69%)] Loss: -11122.948242\n",
      "Train Epoch: 96 [552960/663552 (83%)] Loss: -11122.654297\n",
      "Train Epoch: 96 [645120/663552 (97%)] Loss: -11123.055664\n",
      "    epoch          : 96\n",
      "    loss           : -11122.95105432581\n",
      "    val_loss       : -11122.962646484375\n",
      "Train Epoch: 97 [0/663552 (0%)] Loss: -11123.085938\n",
      "Train Epoch: 97 [92160/663552 (14%)] Loss: -11122.986328\n",
      "Train Epoch: 97 [184320/663552 (28%)] Loss: -11122.920898\n",
      "Train Epoch: 97 [276480/663552 (42%)] Loss: -11123.041992\n",
      "Train Epoch: 97 [368640/663552 (56%)] Loss: -11122.970703\n",
      "Train Epoch: 97 [460800/663552 (69%)] Loss: -11123.074219\n",
      "Train Epoch: 97 [552960/663552 (83%)] Loss: -11123.236328\n",
      "Train Epoch: 97 [645120/663552 (97%)] Loss: -11122.948242\n",
      "    epoch          : 97\n",
      "    loss           : -11122.951048297648\n",
      "    val_loss       : -11122.962836371527\n",
      "Train Epoch: 98 [0/663552 (0%)] Loss: -11122.953125\n",
      "Train Epoch: 98 [92160/663552 (14%)] Loss: -11122.839844\n",
      "Train Epoch: 98 [184320/663552 (28%)] Loss: -11122.788086\n",
      "Train Epoch: 98 [276480/663552 (42%)] Loss: -11122.895508\n",
      "Train Epoch: 98 [368640/663552 (56%)] Loss: -11122.879883\n",
      "Train Epoch: 98 [460800/663552 (69%)] Loss: -11122.733398\n",
      "Train Epoch: 98 [552960/663552 (83%)] Loss: -11122.917969\n",
      "Train Epoch: 98 [645120/663552 (97%)] Loss: -11122.901367\n",
      "    epoch          : 98\n",
      "    loss           : -11122.951114607446\n",
      "    val_loss       : -11122.962456597223\n",
      "Train Epoch: 99 [0/663552 (0%)] Loss: -11123.089844\n",
      "Train Epoch: 99 [92160/663552 (14%)] Loss: -11123.027344\n",
      "Train Epoch: 99 [184320/663552 (28%)] Loss: -11122.944336\n",
      "Train Epoch: 99 [276480/663552 (42%)] Loss: -11122.729492\n",
      "Train Epoch: 99 [368640/663552 (56%)] Loss: -11122.997070\n",
      "Train Epoch: 99 [460800/663552 (69%)] Loss: -11122.997070\n",
      "Train Epoch: 99 [552960/663552 (83%)] Loss: -11122.776367\n",
      "Train Epoch: 99 [645120/663552 (97%)] Loss: -11122.905273\n",
      "    epoch          : 99\n",
      "    loss           : -11122.9511387201\n",
      "    val_loss       : -11122.962863498264\n",
      "Train Epoch: 100 [0/663552 (0%)] Loss: -11122.996094\n",
      "Train Epoch: 100 [92160/663552 (14%)] Loss: -11122.910156\n",
      "Train Epoch: 100 [184320/663552 (28%)] Loss: -11123.049805\n",
      "Train Epoch: 100 [276480/663552 (42%)] Loss: -11123.044922\n",
      "Train Epoch: 100 [368640/663552 (56%)] Loss: -11122.839844\n",
      "Train Epoch: 100 [460800/663552 (69%)] Loss: -11122.957031\n",
      "Train Epoch: 100 [552960/663552 (83%)] Loss: -11122.860352\n",
      "Train Epoch: 100 [645120/663552 (97%)] Loss: -11122.883789\n",
      "    epoch          : 100\n",
      "    loss           : -11122.950948832948\n",
      "    val_loss       : -11122.962972005209\n",
      "Saving checkpoint: saved/models/Shapes_ChyVAE/0802_175221/checkpoint-epoch100.pth ...\n",
      "Saving current best: model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HTFATorch] *",
   "language": "python",
   "name": "conda-env-HTFATorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
